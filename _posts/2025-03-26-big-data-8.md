---
layout: post
title: "[빅분기] 필기 Chapter 08. 분석 기법 적용"
description: "[빅데이터분석기사] 필기 'Chapter 08. 분석 기법 적용' 내용을 정리했습니다."
author: "DoorNote"
date: 2025-03-26 10:00:00 +0900
permalink: /big-data-8/
categories:
    - 자격증
    - 빅데이터분석기사
tags: ['[빅분기]-필기', PART 03. 빅데이터 모델링]
use_math: true
comments: true
pin: false # 고정핀
math: true
mermaid: true
image: /assets/img/빅데이터분석기사.png
---

## 들어가며

> **빅데이터분석기사 필기 시험**을 준비하며 공부한 내용을 **Chapter** 별 핵심 내용 기준으로 정리한 내용입니다.<br>
> 교재는 [이기적-빅데이터분석기사 필기-2024](https://search.shopping.naver.com/book/catalog/41869053620?cat_id=50010601&frm=PBOKPRO&query=%EC%9D%B4%EA%B8%B0%EC%A0%81+%EB%B9%85%EB%8D%B0%EC%9D%B4%ED%84%B0%EB%B6%84%EC%84%9D%EA%B8%B0%EC%82%AC+%ED%95%84%EA%B8%B0&NaPm=ct%3Dm88q4ot4%7Cci%3D23875283d14200743013ebc63ca27206ecd57363%7Ctr%3Dboknx%7Csn%3D95694%7Chk%3Dd90241b163c19c2f3cce4b072ef86e71d42f9d26)로 공부했습니다.
{: .prompt-tip }

<br>
<br>

## 01. 분석 기법

---

### 1. 분석 기법 개요

> 데이터 분석 모델은 학습 유형에 따라 **지도학습, 비지도학습, 준지도학습, 강화학습**으로 구분된다.
{: .prompt-info }

<br>

**지도학습**

- **정답(Label)**이 있는 데이터를 활용해 데이터를 학습시키는 방법
- **입력값이 주어질 때 정답이 무엇인지 알려주면서 컴퓨터를 학습**시키는 방법
- 대표적으로 **분류, 회귀**로 구분할 수 있다.

| 구분 | 알고리즘 |
|:---:|:--|
| 분류 | 의사결정트리(분류), 랜덤포레스트, 인공신경망(지도학습), 서포트벡터머신(SVM), 로지스틱 회귀분석 |
| 회귀 | 의사결정트리(회귀), 선형회귀분석, 다중회귀분석 |

<br>

**비지도학습**

- **정답(Label)이 없는 데이터를 컴퓨터 스스로 학습하여 숨겨진 의미, 패턴을 찾아내고 구조화** 하는 방법
- 대표적으로 **군집화, 차원축소, 연관 규칙 분석, 오토인코더, 인공신경망** 등이 있다.

| 구분 | 알고리즘 |
|:----:|:--|
| 군집화 | K-means, DBSCAN, 계층적 군집분석 |
| 차원축소 | PCA, t-SNE |
| 연관 규칙 분석 | Apriori, Eclat |
| 오토인코더 | AutoEncoder |
| 인공신경망 | Self-Organizing Map (SOM), Restricted Boltzmann Machine (RBM) |

<br>

**준지도학습**

- **정답(Label)이 있는 데이터와 정답이 없는 데이터를 동시에 학습에 사용** 하는 방법
- 완전히 **라벨링된 데이터 확보가 어려운 상황**에서 효과적으로 사용됨  
- 라벨링된 **소량의 데이터로 학습을 유도**하고, 라벨이 없는 데이터로 **일반화 성능을 향상시킴**
- 대표적으로 **Self-training, Pseudo-labeling, Graph-based 모델, Consistency Regularization** 등이 있다.

| 구분 | 알고리즘 / 설명 |
|:----:|:--|
| Self-training | 모델이 예측한 라벨을 신뢰도 기반으로 가짜 라벨로 재사용 |
| Pseudo-labeling | 예측 결과 중 확신이 높은 값을 비지도 데이터에 라벨처럼 활용 |
| Graph-based 모델 | 데이터 간 유사도를 그래프로 구성해 레이블 전파 |
| Consistency Regularization | 입력에 변형을 줘도 일관된 예측을 하도록 유도 |

<br>

**강화 학습**

- 주어진 환경에서 보상을 최대화하도록 에이전트를 학습하는 기법
- 명시적인 정답이 주어지지 않으며, **행동(Action)**의 결과에 따라 얻는 보상을 기반으로 최적의 정책을 학습  
- 대표적으로 **Q-Learning, DQN, Policy Gradient, Actor-Critic** 등의 알고리즘이 있다.

| 구분 | 알고리즘 / 설명 |
|:----:|:--|
| Q-Learning | 상태-행동 가치 함수(Q)를 업데이트하며 최적 정책 학습 |
| DQN (Deep Q-Network) | Q-Learning에 딥러닝을 접목한 대표적 심층 강화학습 기법 |
| Policy Gradient | 정책을 직접 학습하는 방식, 연속적인 행동 공간에 적합 |
| Actor-Critic | 정책(Actor)과 가치함수(Critic)를 함께 학습해 안정성 향상 |

<br>
<br>

### 2. 회귀 분석

> 특정 변수가 다른 변수에 어떤 **영향을 미치는지**를 수학적 모형으로 설명, 예측하는 기법으로 **독립변수로 종속변수를 예측**하는 기법으로도 알려져 있다.
{: .prompt-info }

- 독립변수: 입력값 또는 원인을 설명하는 변수
- 종속변수: 결과값 또는 효과를 설명하는 변수
- 회귀선(회귀계수): 종속변수의 **기대값으로** 일반적으로 **최소제곱법을 이용**
- 최소제곱법: 잔차 제곱의 합이 최소가 되게 하는 직선을 찾는 방법

$$
\min \sum_{i=1}^{n} (y_i - \hat{y}_i)^2
$$

#### **2-1) 선형회귀분석**

1. **단순 선형회귀분석**
    - **한 개의 종속변수 y**와 **한 개의 독립변수 x**로 **두 개의 변수 사이의 관계를 분석**

2. **다중 선형회귀분석**
    - **독립변수가 두 개 이상이고 종속변수 y가 하나인** 선형회귀분석

<br>

**선형회귀분석의 기본적인 가정**

| 종류 | 내용 |
|:--:|:--|
| 선형성 | 독립변수와 종속변수가 **선형적**이어야 한다. |
| 잔차 정규성 | 잔차의 **기댓값은 0이며 정규분포**를 이루어야 한다. |
| 잔차 독립성 | 잔차들은 서로 **독립적이어야** 한다. |
| 잔차 등분산성 | 잔차들의 **분산이 일정해야** 한다. |
| 다중 공선성 | 다중 회귀분석을 실행할 경우 **3개 이상의 독립변수 간에 상관관계로 인한 문제가 없어야 한다.** |

<br>

#### **2-2) 로지스틱 회귀분석**

> 선형회귀분석과 유사하나, 종속변수가 연속형이 아닌 **범주형으로** 입력 데이터가 주어졌을 때 특정 **분류로** 결과가 나타나는 것이 다른 점이다.
{: .prompt-info }

- 선형회귀분석과의 차이점은 종속변수를 **범주형으로** 확장하였고
- **정규분포가** 아닌 **이항분포를** 따른다.

1. **단순 로지스틱 회귀분석**
    - 종속변수가 이항형 **문제(범주의 개수가 두 개인 경우)**인 회귀분석

2. **다중 로지스틱 회귀분석**
    - 종속변수가 이항형 문제가 아닌 **두 개 이상의 범주를 가지게 될 경우의 회귀분석**

<br>

#### **2-3) 회귀분석의 장단점**

- 장점: **크기와 관계없이** 계수들에 대한 명료한 해석과 손쉬운 통계적 **유의성 검증이 가능**
- 단점: **선형적인** 관계로 데이터가 구성되어 있어야 적용할 수 있다.

<br>
<br>

### 3. 의사결정나무(Decision Tree)

> **의사결정 규칙을 나무 모양으로 조합하여 목표 변수에 대한 분류 또는 예측을 수행**<br>
부모마디보다 자식마디의 순수도(purity) 증가, 불확실성은 감소하도록 분리 진행(정보 획득)
{: .prompt-info }

#### **3-1) 의사결정나무의 종류**

- 의사결정나무는 목표변수가 **이산형인 경우의 분류나무**와 목표변수가 **연속형인 경우의 회귀나무**로 구분

<br>

#### **3-2) 의사결정나무의 분석 과정**

1. **변수 선택** : 목표변수와 관련된 설명(독립) 변수들을 선택
2. **의사결정나무 형성** : 분석목적에 따라 적절히 훈련데이터를 활용
3. **가지치기** : 부적절한 추론규칙을 가지거나 불필요 또는 분류오류를 크게 할 위험이 있는 마디 제거
4. **타당성 평가** : 이익, 비용, 위험 등을 고려하여 모형을 평가
5. **해석 및 예측** : 최종 모형에 대한 해석으로 분류 및 예측 모델을 결정

<br>

**가지치기**

<img src="/assets/img/Dicision-Tree.png" width="80%">

<br>

#### **3-3) 의사결정나무의 대표적 알고리즘**

**랜덤 포레스트(Random Forest)**

- **부트스트래핑** 기반 샘플링을 활용한 의사결정나무
- 생성 이후 **배깅 기반 나무들을 모아 앙상블 학습하여 숲을 형성하게** 되면 이를 랜덤포레스트라고 함

<br>

- **앙상블**
    - **여러 모델을 학습시켜 결합**하는 방식의 학습방법
    - **일반화 성능을 향상시켜 과적합**을 해결할 수 있음

- **배깅**
    - 여러 **부트스트랩** 자료를 생성하여 학습하는 모델
    - **분류기**를 생성한 후 그 결과를 **앙상블** 하는 방법 

- **부스팅**
    - 가중치를 활용해 **약분류기를 강분류기로 만드는 방법**
    - 분류 모델들이 **틀린 곳에 집중하여 새로운 분류 규칙을 생성**하는 기법

<img src="/assets/img/배깅-부스팅-차이.png" width="80%">

- **의사결정나무 장단점**
    - 장점 
        - **연속형, 범주형** 변수 모두 적용, **변수 비교가 가능하며 규칙에 대해 이해하기 쉬움**
        - 규칙을 도출하는 데에 유용하고 **마케팅, CRM, 시장조사, 기업 부도/환율 예측** 등 활용

    - 단점
        - 트리구조가 복잡할 시 **예측/해석력이 떨어짐**
        - **데이터 변형에 민감**

<br>
<br>

### 4. 인공신경망

> 인공신경망은 **인간의 두뇌 신경세포인 뉴런**을 기본으로 한 **기계학습** 기법으로 하나의 뉴런이 다른 뉴런들과 **연결되어 신호를 절단**, 처리하는 구조를 본떴다.
{: .prompt-info }

#### **4-1) 인공신경망의 원리**

<img src="/assets/img/Deep-Learning.png" width="100%">

- 노드: 신경계 뉴런, 가중치와 입력값으로 활성화함수를 통해 다음 노드로 전달
- 가중치: 노드와의 연결계수
- 활성함수: 임계값을 이용, 노드의 활성화 여부를 결정
- 입력층: 학습 위한 데이터 입력
- 은닉층: 다층 네트워크에서 입력층과 출력층 사이
- 출력층: 결과 출력

<br>

#### **4-2) 학습**

> 신경망에는 적응 가능한 **가중치**와 **편향**이 있으며 이를 훈련 데이터에 적응하도록 조정하는 과정을 학습이라고 정의한다.
{: .prompt-info }

**손실함수**

- 신경망이 출력한 값과 실제 값과의 오차에 대한 함수
- 일반적인 손실 함수로는 평균제곱 오차 또는 교차엔트로피 오차를 활용

<br>

**평균제곱 오차(MSE: Mean Squared Error)**

- 출력 값과 사용자가 원하는 출력 값 사이의 거리 차이를 오차로 사용 
- 각 거리 차이를 제곱하여 합산한 후 평균을 구한다.

<img src="/assets/img/MSE.png" width="80%">

<br>

**교차엔트로피 오차(CEE: Cross Entropy Error)**

- 분류 부문으로 t값이 원-핫 인코딩 벡터 
- 모델의 출력 값에 자연로그를 적용, 곱한다.

<img src="/assets/img/Cross-Entropy.png" width="70%">

<br>

**학습 알고리즘**

1. 미니배치
    - 훈련 데이터 중 일부를 무작위로 선택한 데이터
    - 손실함수를 줄이는 것을 목표로 설정

2. 기울기 산출
    - 미니배치의 손실함수 값을 최소화하기 위해 경사법으로 가중치 매개변수의 기울기를 미분을 통해 구한다.
    - 기울기의 최소값을 찾는 경사 하강법, 경사 상승법, 무작위 미니배치를 통한 확률적 경사 하강법이 있다.

3. 매개변수 갱신
    - 가중치 매개변수를 기울기 방향으로 조금씩 업데이트 하면서 1~3단계를 반복

<br>

**오차역전파**

- **모델이 틀린 이유를 거꾸로 추척해가며 스스로 학습하는 과정**
- 오차를 출력층에서 입력층으로 전달, 연쇄법칙을 활용한 역전파를 통해 가중치와 편향을 계산, 업데이트한다.
- 신경망 각 계층에서의 역전파 처리는 **렐루, 시그모이드 계층, 아핀 계층, Softmax-with-Loss** 등이 있다.

<br>

**활성화 함수**

- 입력 신호의 **총합**을 그대로 사용하지 않고 **출력 신호로 변환하는 함수**
- 대표적인 함수로는 **Relu, Sigmoid** 가 있다.
- **퍼셉트론**은 1개 이상의 입력층과 1개 출력층 뉴런으로 구성된 활성화 함수에 따라 출력되는 신경구조망
- **다층 퍼셉트론**은 **은닉층**이 1개 이상의 퍼셉트론으로 활성화함수인 계단 함수를 사용하여 0 또는 1을 반환

| 구분 | 설명 | 예시 / 특징 |
|:--:|:--|:--|
| **활성화 함수** | 입력 신호의 **총합을 변환**하여 출력 신호로 전달하는 함수 | - **ReLU**: 0보다 작으면 0, 크면 그대로 출력<br>- **Sigmoid**: 0~1 사이의 값으로 출력 (확률처럼 해석 가능) |
| **퍼셉트론** | 1개 이상의 입력값을 받아, 가중합을 기준으로 **출력을 결정**하는 구조 | - 단층 구조<br>- 활성화 함수에 따라 결과가 달라짐 |
| **다층 퍼셉트론 (MLP)** | 퍼셉트론이 여러 층으로 연결된 구조 | - **은닉층(hidden layer)** 존재<br>- 복잡한 문제도 해결 가능<br>- 계단 함수나 ReLU 등 사용 |

<br>

**과적합**

- 일반적으로 훈련데이터가 적은 경우, 매개변수가 많고 표현력이 높은 모델인 경우 등에 발생
- 해결방안
    - **가중치 감소(Weight Decay)**
    - **드롭아웃(Dropout)**
    - **하이퍼라마이터 최적화**

<br>

1. **가중치 감소(Weight Decay)**
    - **가중치가 클수록 일종의 패널티를 부과**하여 가중치 매개변수 절대값을 감소시켜 과적합의 위험을 줄인다.
    - 패널티 역할로 규제가 이용된다.
    - 모델을 강제로 제한한다는 의미이며 **L1=Lasso, L2=Ridge** 규제가 있다.

2. **드롭아웃(Dropout)**
    - 신경망의 **과대적합을 줄이고 일반화 성능을 향상시킬 수 있는 효과가 있다.**
    - 훈련 시에만 적용되며, **테스트나 추론 단계에서는 사용되지 않는다.**

3. **하이퍼파라미터 최적화**
    - 최적의 딥러닝 구현을 위해 **학습률, 배치크기, 훈련 반복 횟수, 가중치** 등을 수동으로 설정하는 변수
    - 최적의 값은 모델이나 데이터에 따라 다를 수 있다.

<br>
<br>

### 5. 딥러닝 모델 종류

#### **5-1) CNN(합성곱 신경망 모델)**

- **사람의 시신경 구조를 모방한 구조**로 모든 입력 데이터들을 **동등한 뉴런으로 처리**
- 데이터 특징, 차원을 추출하여 패턴을 이해하는 방식
- 이미지의 **특징을 추출하는 과정(합성곱, 계층, 풀링 계층)과 클래스를 분류**하는 과정으로 진행
- CNN은 보편적으로 **수치, 텍스트, 음성 이미지**들의 여러 유형의 데이터 들에서 많은 특징들을 **자동으로 학습하여 추출, 분류, 인식 처리하는데 사용되고 있다.**

<img src="/assets/img/CNN.png" width="90%">

<br>

**CNN의 매개변수**

| 매개변수 | 설명 |
|:---:|:--|
| **필터(Filter)** | 입력 데이터에 적용되는 작은 행렬로, 이미지의 특징을 추출하는 역할을 한다.(**Kernel** 이라고도 불림) |
| **스트라이드(Stride)** | 필터가 입력 데이터를 얼마나 이동할지를 결정하는 값으로, 이동 거리를 조절하여 출력 데이터의 크기를 조절할 수 있다. |
| **패딩(Padding)** | 입력 데이터 주변을 특정 값으로 채워서 출력 데이터의 크기를 조절하는 것으로, 주로 0으로 채우는 제로 패딩을 사용한다. |

<br>

#### **5-2) RNN**

- 순서를 가진 데이터를 입력하여 단위 간 연결이 시퀀스를 따라 **방향성 그래프를 형성하는 신경네트워크 모델**
- 내부 상태(메모리)를 이용하여 입력 시퀀스를 처리한다.
- **CNN**과는 달리 **중간층이 순환구조로 동일한 가중치를 공유**
- 가중치와 편향에 대한 오차함수의 미분을 계산하기 위해 **확률적 경사하강법**을 이용
- 가중치 업데이트를 위해 **과거시점까지 역전파하는 BPTT를 활용**

<img src="/assets/img/RNN.png" width="90%">

<br>

#### **5-3) LSTM**

- **LSTM**은  **RNN**의 **단점을 보완하기 위해** 변형된 알고리즘
- **보통 신경망 대비 4배 이상 파라미터를 보유하여** 많은 단계를 거치더라도 **오래 시간동안 데이터를 잘 기억한다.**
- **3가지 게이트**로 **입력(Input Gate), 출력(Out-put Gate), 망각(Forget Gate)**로 보완된 구조를 통해 가중치를 곱한 후 **활성화 함수를 거치지 않고** 상황에 맞게 값을 조절함으로써 문제를 해결한다.

<img src="/assets/img/LSTM.png" width="90%">

<br>

#### **5-4) Auto-encoder**

- 대표적 **비지도학습 모델** 
- 다차원 데이터를 저차원으로 바꾸고 바꾼 저차원 데이터를 다시 고차원 데이터로 바꾸면서 특징을 찾아낸다.
- **encoder**를 통해 차원을 줄이는 **hidden layer**로 보내고 **hidden layer**의 데이터를 **decoder**를 통해 차원을 늘리는 출력층으로 내보낸다.
- 하나의 신경망을 두 개 붙여높은 형태이며 **출력 계층과 입력 계층의 차원은 같다.**

<img src="/assets/img/Auto-encoder.png" width="90%">

**오토인코더 종류**

| 모델 종류 | 내용 |
|:---:|:---|
| **디노이징 오토인코더** | 손상이 있는 입력값을 받아도 손상을 제거하고 원본의 데이터를 출력값으로 만든다. |
| **희소 오토인코더** | 은닉층 중 매번 일부 노드만 학습하여 과적합 문제를 해결 |
| **VAE** | 확률분포를 학습함으로써 데이터를 생성 |

<br>

#### **5-5) GAN**

- 학습 데이터 패턴과 유사하게 만드는 **생성자 네트워크와 패턴의 진위 여부를 판별하는 판별자 네트워크로 구성**
- **지도학습(판별자)과 비지도학습(생성자) 딥러닝 모델의 결합으로 서로 경쟁하며 패턴을 흉내**
- 두 네트워크가 서로의 목적을 달성하도록 학습을 반복
    - **판별자 네트워크**
        - **랜덤 노이즈 m 개를 생성**
        - 판별자 네트워크의 정확도를 최대화하는 방향으로 학습

    - **생성자 네트워크**
        - **랜덤 노이즈 m 개를 재생성**하여 생성자가 판별자의 **정확도를 최소화하도록 학습**

<img src="/assets/img/GAN.png" width="100%">

<br>

#### **5-6) 인공신경망의 장단점**

- **장점**
    - 비선형적 예측이 가능
    - 다양한 데이터 유형, 새로운 학습 환경, 불완전한 데이터 입력 등에도 적용 가능

- **단점**
    - 데이터가 커질수록 학습시키는 데에 시간 비용이 **기하급수적**으로 커질 수 있다.
    - 모델에 대한 **설명기능**이 떨어지거나 **설명가능한 AI 등 대체안이 연구되고 있다.**

<br>
<br>

### 6. SVM(Support Vector Machine)

> **서포트벡터머신은(SVM)** **지도학습** 기법으로 고차원 또는 무한 차원의 공간에서 초평면을 찾아 이를 이용하여 분류, 회귀를 수행한다.
{: .prompt-info }

#### **6-1) SVM 주요 요소**

- **벡터(Vector)**: 점들 간 클래스
- 결정영역: 클래스들을 잘 분류하는 선
- **초평면(Hyperplane)**: 서로 다른 분류에 속한 데이터들 간 거리를 가장 크게 하는 분류 선
- **서포트벡터(Support Vector)**: 두 클래스를 구분하는 경계
- **마진(Margin)**: 서포트벡터를 지나는 초평면 사이의 거리

<br>

#### **6-2) SVM 장단점**

- **장점**
    - 다양한 라이브러리로 사용하기 쉬우며 **분류, 회귀 예측 문제에 동시에 활용할 수 있다.**
    - SVM은 **선형 분류와 더불어 비선형 분류에서도 사용될 수 있다.**
    - 신경망 기법에 비해 **적은 데이터로 학습이 가능하며 과적합, 과소적합 정도가 덜하다.**

- **단점**
    - **이진분류**만 가능하며 데이터가 많을 시 모델 학습 시간이 오래 소요된다.
    - 각각 분류에 대한 SVM 모델 구축이 필요

<br>
<br>

### 7. 연관성분석

> 둘 이상의 거래, 사건에 포함된 항목들의 **관련성을 파악하는 탐색적 데이터 분석 기법(EDA)**으로 **컨텐츠 기반 추천의 기본 방법론**으로도 알려져 있다. 그룹에 대한 특성 분석으로 **군집분석과 병행 가능하며 장바구니 분석으로도 불린다.**
{: .prompt-info }

#### **7-1) 연관규칙 순서**

- 지지도(Support): 데이터 전체에서 해당 사건이 나타나는 확률
- 신뢰도(Confidence): 어떠한 사건이 다른 사건에 대하여 나타나는 확률
- 향상도(lift): 두 규칙의 상관관계, 독립인지 판단하는 개념

<br>

#### **7-2) 아프리오리(Apriori) 알고리즘**

- **최소 지지도 이상의 빈발항목집합만 찾아내서 연관규칙을 계산**

<br>

#### **7-3) 연관성분석의 장단점**

- **장점**
    - 분석 결과가 **이해하기 쉽고 실제 적용하기 용이**

- **단점**
    - 품목이 많아질수록 연관성 규칙이 더 많이 발견되나 **의미성에 대해 사전 판단이 필요**
    - 상당 수의 계산과정이 필요

<br>
<br>

### 8. 군집분석

> **비지도학습**의 일종으로, 각 개체들의 유사성을 분석해서 높은 대상끼리 일반화된 그룹으로 분류, **이상치에 민감하
여 신뢰성과 타당성 검증이 어려움**, 사전 정보 없이 특정 패턴·속성 파악에 효과적
{: .prompt-info }

#### **8-1) 군집분류시 기본적인 가정**

- 하나의 군집 내에 속한 개체들의 특성은 동일하다.
- 군집의 개수 또는 **구조와 관계없이 개체간의 거리를 기준으로 분류**
- 개별 군집의 특성은 군집에 속한 개체들의 **평균값으로** 나타낸다.

<br>

#### **8-2) 군집분석의 척도**


| **척도** | **설명** |
|:---:|---|
| **유클리드 거리** | 2차원 공간에서 두 점 간의 가장 짧은 거리 개념, 피타고라스 정리 |
| **맨해튼 거리** | 택시 거리, 시가지 거리, 가로지르지 않고 도착하는 최단거리 |
| **민코프스키 거리** | m=1일 때 맨해튼 거리, m=2일 때 유클리드 거리와 같음 |
| **마할라노비스 거리** | 특정 값이 얼마나 평균에서 멀리 있는지를 나타냄, 변수 간 상관관계 고려 |
| **자카드 거리** | 두 집합 간 비유사성을 측정하는 지표 |

<br>

#### **8-3) 군집분석 장단점**

- **장점**
    - **다양한 데이터 형태에 적용이 가능**
    - 특정 변수에 대한 정의가 필요하지 않는 적용이 용이한 탐색적 기법

- **단점**
    - 초기 군집 수, 관측시간의 거리 등의 결정에 따라 **결과가 바뀔 수 있다.**
    - 사전 주어진 목표가 없으므로 **결과 해석이 안된다.**

<br>
<br>
<br>

## 02. 고급 분석기법

---

### 1. 범주형 자료분석

- **빈도분석** : **질적자료**를 대상으로 **빈도와 비율**을 계산할 때 쓰임  
- **카이제곱검정** : **두 범주형 변수**가 서로 **상관이 있는지**, **독립인지**를 판단하는 **통계적 검정 방법**  
- **로지스틱 회귀분석** : 분석하고자 하는 대상들이 **두 집단 또는 그 이상의 집단**으로 나누어진 경우, 개별 관측치들이 **어느 집단으로 분류될 수 있는지** 분석할 때 사용  
- **T 검정** : 독립변수가 **범주형(두 개의 집단)**이고, **종속변수가 연속형**인 경우 사용되는 **검정 방법**  
- **분산분석** : 독립변수가 **범주형(두 개 이상 집단)**이고, **종속변수가 연속형**인 경우 사용되는 **검정 방법**

<br>
<br>

### 2. 다변량분석

- **다중회귀분석** : **다수의 독립변수** 변화에 따른 **종속변수의 변화**를 **예측**  
- **다변량분산분석** : **두 개 이상의 범주형 종속변수**와 **다수의 계량적 독립변수** 간 관련성을 **동시에** 알아볼 때 이용되는 **통계적 방법**  
- **다변량공분산분석** : 실험에서 **통제되지 않은 독립변수들의 종속변수들에 대한 효과를 제거**하기 위해 이용되는 방법  
- **정준상관분석** : **종속변수군과 독립변수군 간 상관을 최대화**하는 각 변수군의 **선형조합**을 찾음  
- **요인분석** : 많은 변수들 간 **상호관련성**을 분석하고, 어떤 **공통 요인들로 설명**하고자 할 때 이용  
- **군집분석** : **집단에 관한 사전정보 없이**, 각 표본에 대하여 **그 분류체계**를 찾음  
- **다중판별분석** : **종속변수가 비계량적 변수**인 경우, 집단 간 **차이를 판별**하고, **어떤 사례가 특정 집단에 속할 가능성**을 **계량적 독립변수에 기초해 예측**  
- **다차원척도법** : 개체들을 **원래보다 낮은 차원의 공간상에 위치시켜**, 개체들 사이의 **구조 또는 관계**를 **쉽게 파악**하는 목적

<br>
<br>

### 3. 시계열분석

- 추세성분, 계절성분, 순환성분, 복합성분, 자기상관성, 백색잡음
- 정상성(stationarity) : **시계열 데이터가 평균과 분산이 일정한 경우, 분석이 용이한 형태**<br>
→ 모든 시점의 평균과 분산이 일정, 공분산이 시차에만 의존, **정상시계열은 평균회귀 경향을 가짐**

<br>

| 분석 방법       | 설명 |
|----------------|------|
| **단순 방법**   | - **이동평균법** – 일정기간을 시계열을 이동하며 평균을 계산<br> - **지수평활법** – 관찰기간 제한 없이 모든 시계열 데이터를 사용, 최근 시계열에 더 많은 가중치를 줌<br> - **분해법** – 시계열 자료의 성분 분류대로 분해하는 방법 |
| **모형기반 방법** | - **자기회귀모형** – 과거의 패턴이 현재자료에 영향을 준다는 가정<br> - **자기회귀이동평균모형** – AR(p) 모형과 MA(q) 모형의 결합형태<br> - **자기회귀누적이동평균모형** – 비정상성을 가지는 시계열 데이터 분석 |

<br>
<br>

### 4. 베이즈 기법

- 회귀분석모델 적용 : 추정치와 실제의 차이를 **최소화하는 것이 목표**
- 나이브 베이즈 분류 : 일부 확률모델에서 나이브 베이즈 분류는 **지도 학습 환경에서 매우 효율적으로 훈련될 수 있다.**

<br>
<br>

### 5. 딥러닝 분석

- 인공신경망 : 시냅스의 결합으로 네트워크를 형성한 **인공 뉴런(노드)**이 학습을 통해 시냅스의 결합
세기를 변화시켜 **문제 해결 능력을 가지는 모델 전반**
- 심층 신경망(DNN) : 입력층과 출력층 사이에 **여러 개의 은닉층들로 이루어진 인공 신경망**
- 합성곱 신경망(CNN) : **최소한의 전처리를 사용하도록 설계된 다계층 퍼셉트론**의 한 종류
- 순환 신경망(RNN) : 인공 신경망을 구성하는 **유닛 사이의 연결이 directed cycle 을 구성**
- 심층 신뢰 신경망(DBN) : **잠재변수의 다중계층으로** 이루어진 심층 신경망

<br>
<br>

### 6. 비정형 데이터 분석

- 비정형 데이터 분석 기본 원리 : 비정형 데이터의 내용 파악과 패턴 발견을 위해 다양한 기법 활용,
정련 과정을 통해 **정형 데이터로 만든 후 데이터 마이닝을 통해 의미있는 정보 발굴**
- 데이터 마이닝 : 데이터 안에서 **통계적 규칙이나 패턴을 분석**하여 가치 있는 정보 추출<br>
→ **텍스트 마이닝, 자연어 처리, 웹 마이닝, 오피니언 마이닝, 리얼리티 마이닝**

<br>
<br>

### 7. 앙상블 분석

- 주어진 자료로부터 **여러 개의 학습 모형을 만든 후 조합하여 하나의 최종 모형을 만드는 개념**
- 약학습기를 통해 강학습기를 만들어내는 과정
- 약학습기 : **무작위 선정이 아닌 성공률이 높은 학습 규칙**

<br>

**앙상블 분석의 종류**

| 앙상블 기법             | 설명 |
|------------------------|------|
| **보팅(voting)**       | 서로 다른 알고리즘 모델을 조합해서 사용, 결과물에 대해 **투표로 결정** |
| **부스팅(boosting)**   | 가중치를 활용해 연속적인 약학습기를 생성하고 이를 통해 **강학습기를 만듦**<br>순차적인 학습을 하며 가중치를 부여해서 오차를 보완, 병렬처리 어려움 |
| **배깅(bagging)**       | 같은 알고리즘 내에서 다른 표본 데이터 조합을 사용<br>**샘플을 여러 번 뽑아 각 모델을 학습시켜 결과물을 집계** → 랜덤 포레스트 |
| **스태킹(stacking)**   | 다양한 모델들의 **예측 결과를 결합** |

<br>
<br>

### 8. 비모수 통계

> 통계학에서 모수에 대한 가정을 전제로 하지 않고 **모집단의 형태에 관계없이 주어진 데이터에서 직접
확률을 계산하여 통계학적 검정을 하는 분석**

- 모집단의 형상이 **정규분포가 아닐 때, 표본이 적을 때, 자료들이 서로 독립적일 때**
- 질적척도로 측정된 자료도 분석 가능, 비교적 신속하고 쉽게 통계량 구할 수 있음
- **부호검정, 윌콕슨 부호순위 검정, 만 휘트니 검정, 크루스칼-왈리스 검정**