---
layout: post
title: "[Upstage AI Lab] 12주차 - DL-Basic"
description: "[Upstage AI Lab] Deep Learning 기본 개념 정리"
author: "DoorNote"
date: 2025-06-13 10:00:00 +0900
#  permalink: //
categories:
    - AI | 인공지능
    - Upstage AI Lab
tags: [Deep Learning, Perceptrons, Neural Networks, MLP, Gradient Descent, Backpropagation]
use_math: true
comments: true
pin: false # 고정핀
math: true
mermaid: true
image: /assets/img/DL-basic.png
---

## 들어가며

> 이번 주차부터 **Deep Learning**에 대한 강의가 시작됐다.  
> 딥러닝의 발전 단계와 기본 개념 등  **Deep Learning**의 전반적인 흐름을 정리했다.
{: .prompt-tip }

<br>
<br>

## Deep Learning 발전 단계

---

### 딥러닝의 발전 5단계

> **딥러닝**의 전체적인 발전 과정을 **총 5단계**로 나누게 되는 과정에 대한 내용이다.  
> **아래의 그림**처럼 다섯 단계로 분류가 되며 단계별로 **딥러닝이 진화**되어 왔다.
{: .prompt-info }

![딥러닝의 발전](/assets/img/DL-발전.png){: width="800" .center}

**주요 5단계**

- Rule Based Programming
- Conventional Machine Learning
- Deep Learning
- Pre-training & Fine-tuning
- Big Model & Zero/Few shot

<br>
<br>

### Step 1: Rule Based Programming

> **규칙 기반 프로그래밍**이라고 하며 사람이 고안한 **IF–THEN** 형태의 규칙으로 분류, 판단 로직을 만든다.  
> **SW1.0** 방식이라고도 부른다.
{: .prompt-info }

![Step-1](/assets/img/DL-Step1.png){: width="500" .center}

**예시**

- 이미지에서 **모양, 색상, 질감** 등 사람이 정의한 특징을 살펴봄  
- 특정 규칙(예: 귀가 뾰족하고, 수염 패턴이 있으면) 충족 시 "고양이"로 분류  
- 모든 **판단 로직과 연산**을 **사람이 직접 설계**

<br>
<br>

### Step 2: Conventional Machine Learning

> 이번 단계에서도 **feature 추출**은 사람이 수행한다.  
> 하지만 **모델 학습·파라미터 최적화**를 기계가 자동으로 수행하는 방식을 **SW2.0**이라 한다.  
> (일부에서는 SW1.0과 SW2.0의 특징을 섞은 중간 단계로 **SW1.5**라 부르기도 한다)
{: .prompt-info }

![Step-2](/assets/img/DL-Step2.png){: width="600" .center}

**예시**

- 이미지에서 **윤곽선, 색상 분포, 질감** 등 사람이 정의한 특징을 추출  
- 추출된 특징을 **벡터**로 변환해 학습 데이터로 입력  
- **SVM**이나 **Random Forest**가 패턴을 학습해 분류 경계 생성  
- 학습된 모델을 사용해 **새로운 데이터**를 예측

<br>
<br>

### Step 3: Deep Learning

> **SW2.0**에서 사람이 수행하던 **feature 정의** 단계를 제거하고, **자동으로 특징을 추출·학습**하는 단계다.  
> 이를 **SW3.0(딥러닝)**이라고도 하며, **end-to-end**로 입력부터 예측·생성까지 처리할 수 있다.
{: .prompt-info }

![Step-3](/assets/img/DL-Step3.png){: width="600" .center}

**예시**

- 원시 이미지(픽셀 값)를 그대로 **Neural Network**에 입력  
- **다층 합성곱 레이어**가 계층적으로 **feature**를 자동 추출  
- **Dense 레이어**가 추출된 특징으로 **분류 경계**를 학습  
- 학습된 모델이 **새로운 이미지**를 **자동으로 예측**

<br>
<br>

### Step 4: Pre-training & Fine-tuning

> 3단계 딥러닝의 한계는 **Task**가 바뀔 때마다 매번 새로운 모델을 처음부터 학습해야 한다는 점이었다.  
> 이를 해결하기 위해 **대규모 데이터**로 미리 학습한 **Pre-training 모델**을 활용한다.  
> 필요할 때마다 해당 모델을 불러와 **Fine-tuning**을 거쳐 원하는 분류·회귀 작업을 수행한다.
{: .prompt-info }

![Step-4](/assets/img/DL-Step4.png){: width="700" .center}

**Pre-training**

- ImageNet 등 **대표적인 대규모 데이터셋**으로 네트워크 전체를 **사전 학습**  
- 모서리·텍스처·패턴 등 **저수준 feature**를 자동 추출하도록 가중치 학습  
- 다양한 downstream Task에서 **기초 표현(feature extractor)**으로 재사용 가능  

<br>

**Fine-tuning**

- Pre-trained 모델의 **상위 레이어**(분류기 헤드 등)를 대상 Task 데이터로 **재학습**  
- 전체 또는 일부 레이어를 **낮은 학습률**로 미세 조정해 빠르게 적응  
- 최소한의 학습 비용으로 Task-specific 성능을 **극대화**

<br>
<br>

### Step 5: Big Model & Zero/Few shot

> **추가 학습 없이** 단일 거대 모델만으로 모든 작업을 처리하는 단계. **(SW3.0)**   
> **0-shot** 또는 **few-shot** 예시만으로 **Task**를 지시하면 즉시 결과를 생성한다.  
> 주로 텍스트 모델에 적용되며, 대표 사례로 **GPT-3**가 있다.  
{: .prompt-info }

![Step-5](/assets/img/DL-Step5.png){: width="800" .center}

- 태스크별 **별도 모델 학습** 불필요  
- Task-specific 데이터 **수집·라벨링** 없이 **즉시 활용**  
- 거대한 단일 모델 하나로 **다양한 문제 해결**  
- **Pre-training & Fine-tuning** 대비 **추가 비용·시간** 대폭 절감

<br>

**Shot 개념**

- **Zero-shot**: 태스크 설명만 제공  
- **One-shot**: 예시 1개 제공  
- **Few-shot**: 소수의 예시(여러 개) 제공

<br>

**In-context learning vs Pre-training & Fine-tuning**

| 항목                | Pre-training & Fine-tuning | In-context Learning |
|--------------------|-----------------------------|---------------------|
| 추가 학습 여부      | 필요                        | 불필요              |
| 데이터 준비        | Task별 대규모 데이터 필요   | 태스크 설명·소량 예시만  |
| 모델 수            | Task별 각각 모델            | 하나의 통합 모델    |
| 적용 비용·시간     | 높음                        | 낮음                |

<br>
<br>
<br>

## Deep Learning Training

---

### 1. Perceptrons (퍼셉트론)

> 퍼셉트론은 **가중치**와 **편향**을 적용한 뒤 **활성화함수**로 **이진 출력**을 내는 **신경망의 최소 단위다.**   
> 이 기본 개념을 이해해야 **다층 퍼셉트론(MLP)과 역전파 학습 과정**을 명확히 파악할 수 있다.  
> (관련 용어는 [딥러닝 기본 용어 정리](/posts/deep-learning-1/)를 참고하자)  
{: .prompt-info }

![Perceptrons](/assets/img/DL-Perceptrons.png){: width="700" .center}


- 퍼셉트론의 연산은 **가중합(z)**을 구하고, 이를 **임계치**와 비교해 이진 출력을 생성하는 두 단계로 이루어진다.
- **threshold**(임계치) 보다 크면 1 아니면 0을 출력
- **bias**가 클수록 출력값이 쉽게 1이 된다.

<br>
<br>

### 2. Multi-Layer Perceptron, MLP (다층 퍼셉트론)

> 여러 **Perceptron**을 **layer**로 쌓아 복잡한 함수를 근사하는 신경망 구조  
> 은닉층을 거칠 떄마다 **비선형 활성화 함수**를 적용해 **복잡한 패턴을 학습할 수 있다.**  
> **입력층 -> 은닉층 -> 출력층** 으로 구성되며 **Fully-Connected Layers** 라고도 불린다.
{: .prompt-info }

![MLP](/assets/img/MLP-1.png){: width="600" .center}

**구성 요소**

- **Input Layer (입력층)**: 원시 데이터를 받아들임  
- **Hidden Layers (은닉층)**: 여러 퍼셉트론을 조합해 특징(feature) 추출  
- **Output Layer (출력층)**: 최종 예측값을 생성  
- **Weights (가중치)**: 각 뉴런 연결의 **중요도**를 결정하는 파라미터. 학습을 통해 최적값으로 조정됨  
- **Bias (편향)**: 뉴런이 활성화되기 위한 **기준점**을 이동시키는 상수항. 학습을 통해 값이 조정됨  
- **Activation Function (활성화 함수)**: 뉴런 출력을 **비선형**으로 변환해 복잡한 패턴 학습을 가능하게 함  

<br>
<br>

### 3. Gradient Descent (경사 하강법)

> **경사 하강법**은 **손실 함수**를 최소화하기 위해 **가중치**와 **편향**을 반복적으로 **조금씩** 업데이트하며  
> 최적의 파라미터를 찾아가는 **반복 최적화 기법**이다.
{: .prompt-info }

![Gradient-Descent](/assets/img/Gradient-Descent.png){: width="600" .center}

**핵심 원리**

- **경사 하강법(Gradient Descent)**은 말 그대로 "**경사(기울기)를 따라 내려가는**" 알고리즘이다.
- **손실 함수(Loss function)**가 가장 작은 지점을 찾기 위해, 현재 지점에서의 **gradient**를 계산하여 조금씩 이동
- **손실 함수의 기울기는 미분으로 구하며**, 손실을 **줄이는 방향으로 파라미터를 조정한다.**
- 간단히 말하면 **가장 가파르게 내려가는 방향을 반복적으로 찾아 움직이며, 결국 최적의 지점으로 수렴**하게 된다.

<br>

**경사 하강법의 종류**

- **배치** 경사 하강법(Batch Gradient Descent)
    - 전체 데이터를 **한 번에 사용해 기울기를 계산**하고 일괄 업데이트를 수행한다.
    - **안정적인 수렴을 보장하지만, 데이터가 많으면 속도가 느려진다.**
- **확률적** 경사 하강법(Stochastic Gradient Descent, SGD)
    - 각 데이터를 **하나씩 사용**할 때마다 파라미터를 업데이트한다.
    - **업데이트 빈도가 높아 불안정할 수 있지만, 학습 속도가 빠르고 빠르게 최적의 근처로 접근할 수 있다.**
    - 볼록ㅏ지 않ㅡ 목ㅓ식을 가진 경우 **SGD**가 **GD**보다 더 낫다고 검증됨
- **미니배치** 경사 하강법(Mini-batch Gradient Descent)
    - 데이터를 **여러 개의 작은 배치로 나누어 사용**하며, 각 배치를 사용해 업데이트한다.
    - **안정성과 속도를 모두 잡을 수 있어 가장 많이 활용된다.**

<br>

**학습률(Learning Rate)의 중요성**

- **학습률**은 한 번에 얼마나 **큰 보폭으로 이동할지 결정**하는 값이다.
- 학습률이 너무 크면 **발산하고**, 너무 작으면 **수렴이 느려진다.**
- **적절한 학습률을 설정**하는 것이 **경사 하강법의 성공을 결정**짓는다.

<br>

**경사 하강법의 한계와 개선 방법**

- **문제점**
    - 지역 최적점(Local minima)에 빠질 수 있다.
    - 안장점(Saddle points) 근처에서 학습이 느려지거나 멈출 수 있다.

- **개선 방법**
    - **Momentum(모멘텀)**: **과거의 기울기**를 이용해 **관성을 추가하여 안장점이나 지역 최적점을 빠르게 탈출**한다.
    - **Adagrad, RMSProp, Adam** 등의 알고리즘은 **학습률을 자동 조정해 최적화를 빠르고 안정적으로 수행한다.**

<br>
<br>

### 4. Backpropagation (역전파)
 
> **Backpropagation**은 **출력층에서 입력층 방향**으로 오차를 전파시키며 각 층의 **가중치를 업데이트**  
> 이 과정을 반복해 **손실을 점진적으로 최소화한다.**
{: .prompt-info }

**등장 배경**

> **MLP** 같은 깊은 신경망 덕분에 이제는 선형적으로 구분되지 않는 데이터까지도 꽤 잘 분류할 수 있게 됐다.  
> 그런데 **신경망이 점점 깊어지면서 문제가 생겼다.** 층이 많아지면 **가중치의 수도 기하급수적으로 늘어나는데**   
> 기존 방식인 경사하강법으로는 이 엄청난 수의 가중치를 일일이 업데이트하려다 보면 연산량이랑 메모리가 너무 많이 든다는 거다. 
> 그래서 나온 아이디어가 바로 **역전파 알고리즘(Backpropagation Algorithm)**이다.

![Backpropagation](/assets/img/Backpropagation.png){: width="700" .center}

**Feed Forward (순전파)**

> **입력층에서 출력층 방향으로 연산이 진행되는 순방향 신경망**(Feed Forward Neural Network)이다.  
> 연산이 **앞으로만 이동하는 과정을 순전파(Feed Forward)라고 부른다.**

![Feed-Forward](/assets/img/Feed-Forward.gif){: width="700" .center}

**Backpropagation (역전파)**

> 역전파의 핵심 아이디어는 사실 생각보다 간단하다. 결국 원하는 건 **신경망의 손실을 줄이는** 건데   
> **출력층에서 발생한 오차를 입력층 방향으로 되돌려서 각 가중치의 책임을 묻는 거다.**  
> 이때 신경망의 가중치마다 얼마나 책임이 있는지 알아내기 위해서, **각 층마다 손실 함수의 미분값을 계산**한다.  
> **즉, 역전파는 단순히 오차를 거꾸로 전달하는 게 아니라**, 각 가중치가 손실에 얼마나 기여했는지를  
> 정확하게 계산해서 **경사 하강법이 필요한 기울기 정보를 빠르고 효율적으로 전달해주는 역할**을 한다.  
> 덕분에 **수많은 가중치를 한 번에, 그리고 효과적으로 업데이트**할 수 있는 거다.  
> 전체적으로 **순전파(Feed Forward)와 역전파(BackPropagation)**의 흐름을 살펴보면 다음과 같다.

![순전파-역전파](/assets/img/순전파-역전파.gif){: width="800" .center}

<br>
<br>
<br>

## Model Optimization

---

### 1. Dropout (드롭아웃)

> **Dropout (드롭아웃)**은 모델 학습 시 **임의의 가중치 노드를 일정 확률로 비활성화** 시키는 방법  
> **학습이 잘 된 모델이라면**, 모델 중 **일부 노드를 비활성화시켜도 좋은 성능을 유지할 것으로 기대**한다.
{: .prompt-info }

![Dropout](/assets/img/Dropout.png){: width="700" .center}

**작동 방식**

- **과적합 방지**를 위해 모델 중간 가중치 **노드에 노이즈를 주면서 학습이 진행되지 않게** 함
- 학습 중에는 **손실함수에 대한 기울기가 흐르지 않아 가중치 업데이트 불가**

**드롭아웃과 앙상블 효과**

- **앙상블(Ensemble)**은 개별적으로 학습시킨 여러 모델의 결과를 종합하여 추론하는 방식이다. (집단 지성, 투표)
- **드롭아웃도 학습 때 뉴런을 무작위로 삭제하는 행위**가 마치 매번 다른 모델을 학습시키는 것으로 해석가능
- **추론 시에는 모든 뉴런을 사용**하므로 **여러 모델의 앙상블로 해석 가능하다.**

<br>
<br>

### 2. Normalization (정규화)

> **정규화(Normalization)**는 딥러닝에서 **학습을 더 빠르고 안정적으로 만들기 위해** 꼭 필요한 과정이다.  
> 입력 데이터나 **은닉층 출력을 일정한 분포**(주로 평균 0, 분산 1)로 맞춰주는 게 핵심이다.
{: .prompt-info }

![Normalization](/assets/img/Normalization.png){: width="800" .center}

**대표적인 정규화 방법**

- **Batch Normalization (배치 정규화)**
    - **배치** 단위의 데이터를 기준으로 평균과 분산을 계산하여 활성화 레이어 이후 출력을 정규화하는 방법이다.
    - 학습을 빠르게 수렴시키고, **내부 공변량 변화**를 줄여주는 효과가 있다.
    - **활성화 함수를 적용하기 전에 적용하는 것을 추천한다.**

<br>

- **Layer Normalization (레이어 정규화)**
    - **배치 정규화의 단점을 보완한 방법** 중 하나다.
    - 배치 정규화와 다르게 **Batch size**에 무관하게 사용 가능
    - 개별 **데이터 샘플 내에서 모든 특징들의 평균과 분산을 계산**하여 정규화한다.
    - **RNN, Transformer** 등에서 많이 쓴다.

<br>

- **Instance Normalization (인스턴스 정규화)**
    - 스타일 변환(Style Transfer) 같은 **이미지 생성 모델**에서 자주 사용된다.
    - **각 샘플의 각 채널마다** 평균과 분산을 구해 정규화하는 방식이다.
    - 배치 크기나 샘플 수에 상관없이 **개별 이미지의 스타일이나 질감을 안정적으로 제어**할 수 있다.
    - 배치 간 상호작용이 없기 때문에 **스타일 일관성 유지에 유리**하다.

<br>

- **Group Normalization (그룹 정규화)**
    - **채널을 여러 그룹으로 나눈 뒤**, 각 그룹 내에서 평균과 분산을 계산해 정규화한다.
    - 배치 크기와 무관하게 동작하므로 **소규모 배치 환경**에서도 안정적으로 학습 가능하다.
    - **Batch Normalization**처럼 글로벌 통계에 의존하지 않고, **Layer Normalization**보다 더 유연한 구조다.
    - 컴퓨터 비전, 특히 **3D 이미지 처리나 작은 배치 학습 상황**에서 유용하게 쓰인다.

<br>

**요약 및 정리**

- 배치 정규화: **배치 단위로 학습 시 발생할 수 있는 분포의 변화를 정규화**하여 모델 학습을 안정화함
- 레이어 정규화: **시계열 데이터**처럼 **입력 구조가 유동적인 경우에도 잘 작동하여 배치 정규화의 한계를 보완함**
- 인스턴스 정규화: **이미지 스타일 변화**와 같이 각 데이터의 개별 특성을 유지해야 할 때 유리함
- 그룹 정규화: **인스턴스 정규화의 확장 버전**으로, **작은 배치 사이즈 환경에서도 안정적으로 작동함**

<br>
<br>

### 3. Weight Initialization (가중치 초기화)

> **가중치 초기화**는 신경망 학습의 시작점에서 가중치 값을 어떻게 설정할지 결정하는 과정이다.  
> 잘못된 **초기값은 학습을 느리게 하거나 수렴하지 않게 만들 수도 있기 때문에** 초기화 전략이 매우 중요
{: .prompt-info }

![Weight-초기화](/assets/img/Weight-초기화.png){: width="1000" .center}

**필요한 이유**

- 초기화를 진행하지 않으면 **모델의 층이 깊이질수록 활성화 함수 이후 데이터 분포가 한 쪽으로 쏠릴 수 있으며**
- **손실 함수(Loss Function)**로부터 기울기 값이 **점점 작아지거나 커질 수 있다.**
- 이러한 현상은 **오차 역전파로 가중치를 갱신해야하는 모델의 학습을 효율적으로 진행되지 않게 만듬**

<br>

**초기화 시 주의할 점**

> **0으로 초기화를 하면** 손실함수로부터의 기울기가 있어도 업데이트가 안 되기 때문에 **가장 최악의 방법이다.**  
> **균등하게 가중치를 초기화하면** 고르게 잘 퍼지지만, 출력 값의 **범위가 너무 넓은** 것을 확인할 수 있다.  
> **표준정규분포로 가중치를 초기화하면** 가중치의 값이 너무 크거나 너무 작게 초기화 되어 **학습에 방해됨**
{: .prompt-warning }

<br>

**Xavier** 초기화

![Xavier-초기화](/assets/img/Xavier-초기화.png){: width="1000" .center}

- 대표적인 가중치 초기화 중 하나로, **Sigmoid**나 **Tanh** 같은 **선형 활성화 함수 또는 근사를 사용할 때 효과적이다.**
- 이전 레이어의 노드 수에 **비례하여 가중치를 초기화**하며
- 이를 통해 **이전 레이어의 노드 수가 많을수록 각 가중치의 크기는 작아지게** 된다.

<br>

**He** 초기화

![He-초기화](/assets/img/He-초기화.png){: width="700" .center}

- Xavier 초기화는 **특정한 활성화 함수에선 여전히 문제를 해결할 수 없었다. (ex: ReLU)**
- **He 초기화**는 이러한 **치명적인 문제를 완화시키려는 목적으로 제안**되었다. 
- **He 초기화**도 Xavier 초기화와 **동일하게 이전 레이어의 노드 수에 비례하여 가중치를 초기화한다.**
- 하지만 He 초기화는 **ReLU 활성화 함수의 특성** 때문에 **더 큰 스케일을 사용한다.**

<br>

> **Sigmoid** 같은 활성화 함수를 쓰게 된다면, **Xavier 초기화** 방법을 사용 (추천)  
> **ReLU**와 같은 활성화 함수를 쓰게 된다면, **He 초기화** 방법을 사용 (추천)
{: .prompt-tip }

<br>
<br>

### 4. Weight Decay (가중치 감쇠)

> **가중치 감쇠(Weight Decay)**는 학습 중 가중치 값이 과도하게 커지는 것을 방지해주는 **정규화 기법이다.**
> 모델이 지나치게 복잡해지는 것을 막아 **과적합(overfitting)**을 완화하는 데 도움을 준다.
{: .prompt-info }

![Weight-Decay](/assets/img/Weight-Decay.png){: width="600" .center}

- 훈련 데이터가 적거나, 훈련 데이터에 노이즈가 많은 경우에 유용
- 주로 L2 정규화의 형태로 적용된다.
- **Dropout처럼 일반화 성능 향상에 효과적이며, 적절한 weight_decay 값을 찾는 것이 중요**

```python
# PyTorch 예시
optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=1e-5)
```

<br>
<br>

### 5. Early stopping (조기 종료)

> **Early stopping (조기 종료)**는 모델 학습 시 **과적합을 방지**해주는 방법 중 하나  
> 학습 오차는 줄어들지만 **검증 오차가 줄어들지 않고 늘어나는 시점에서 학습을 중지**하는 방법이다.
{: .prompt-info }

![Early-stopping](/assets/img/Early-stopping.png){: width="600" .center}

> 학습이 계속되면서 **Train Loss는 감소하더라도, Val Loss가 증가하는 시점**을 감지하여    
> **더 이상 일반화 성능이 향상되지 않는다고 판단되면 학습을 조기에 중단**하는 전략이다.  
> 과적합을 방지하고 **시간 낭비를 줄일 수 있다는 점에서 매우 자주 활용된다.**  
> 보통은 `patience` 값을 설정해 **몇 epoch 동안 개선이 없을 때 멈출지를 조절**한다.