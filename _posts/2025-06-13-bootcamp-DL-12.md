---
layout: post
title: "[Upstage AI Lab] 12주차 - DL-Basic"
description: "[Upstage AI Lab] Deep Learning 기본 개념 정리"
author: "DoorNote"
date: 2025-06-13 10:00:00 +0900
#  permalink: //
categories:
    - AI | 인공지능
    - Upstage AI Lab
tags: [Deep Learning, Neural Networks, MLP, Gradient Descent, Backpropagation]
use_math: true
comments: true
pin: false # 고정핀
math: true
mermaid: true
image: /assets/img/DL-basic.png
---

## 들어가며

> 이번 주차부터 **Deep Learning**에 대한 강의가 시작됐다.  
> 딥러닝의 발전 단계와 기본 개념 등  **Deep Learning**의 전반적인 흐름을 정리했다.
{: .prompt-tip }

<br>
<br>

## Deep Learning 발전 단계

---

### 딥러닝의 발전 5단계

> **딥러닝**의 전체적인 발전 과정을 **총 5단계**로 나누게 되는 과정에 대한 내용이다.  
> **아래의 그림**처럼 다섯 단계로 분류가 되며 단계별로 **딥러닝이 진화**되어 왔다.
{: .prompt-info }

![딥러닝의 발전](/assets/img/DL-발전.png){: width="800" .center}

**주요 5단계**

- Rule Based Programming
- Conventional Machine Learning
- Deep Learning
- Pre-training & Fine-tuning
- Big Model & Zero/Few shot

<br>
<br>

### Step 1: Rule Based Programming

> **규칙 기반 프로그래밍**이라고 하며 사람이 고안한 **IF–THEN** 형태의 규칙으로 분류, 판단 로직을 만든다.  
> **SW1.0** 방식이라고도 부른다.
{: .prompt-info }

![Step-1](/assets/img/DL-Step1.png){: width="500" .center}

**예시**

- 이미지에서 **모양, 색상, 질감** 등 사람이 정의한 특징을 살펴봄  
- 특정 규칙(예: 귀가 뾰족하고, 수염 패턴이 있으면) 충족 시 "고양이"로 분류  
- 모든 **판단 로직과 연산**을 **사람이 직접 설계**

<br>
<br>

### Step 2: Conventional Machine Learning

> 이번 단계에서도 **feature 추출**은 사람이 수행한다.  
> 하지만 **모델 학습·파라미터 최적화**를 기계가 자동으로 수행하는 방식을 **SW2.0**이라 한다.  
> (일부에서는 SW1.0과 SW2.0의 특징을 섞은 중간 단계로 **SW1.5**라 부르기도 한다)
{: .prompt-info }

![Step-2](/assets/img/DL-Step2.png){: width="600" .center}

**예시**

- 이미지에서 **윤곽선, 색상 분포, 질감** 등 사람이 정의한 특징을 추출  
- 추출된 특징을 **벡터**로 변환해 학습 데이터로 입력  
- **SVM**이나 **Random Forest**가 패턴을 학습해 분류 경계 생성  
- 학습된 모델을 사용해 **새로운 데이터**를 예측

<br>
<br>

### Step 3: Deep Learning

> **SW2.0**에서 사람이 수행하던 **feature 정의** 단계를 제거하고, **자동으로 특징을 추출·학습**하는 단계다.  
> 이를 **SW3.0(딥러닝)**이라고도 하며, **end-to-end**로 입력부터 예측·생성까지 처리할 수 있다.
{: .prompt-info }

![Step-3](/assets/img/DL-Step3.png){: width="600" .center}

**예시**

- 원시 이미지(픽셀 값)를 그대로 **Neural Network**에 입력  
- **다층 합성곱 레이어**가 계층적으로 **feature**를 자동 추출  
- **Dense 레이어**가 추출된 특징으로 **분류 경계**를 학습  
- 학습된 모델이 **새로운 이미지**를 **자동으로 예측**

<br>
<br>

### Step 4: Pre-training & Fine-tuning

> 3단계 딥러닝의 한계는 **Task**가 바뀔 때마다 매번 새로운 모델을 처음부터 학습해야 한다는 점이었다.  
> 이를 해결하기 위해 **대규모 데이터**로 미리 학습한 **Pre-training 모델**을 활용한다.  
> 필요할 때마다 해당 모델을 불러와 **Fine-tuning**을 거쳐 원하는 분류·회귀 작업을 수행한다.
{: .prompt-info }

![Step-4](/assets/img/DL-Step4.png){: width="700" .center}

**Pre-training**

- ImageNet 등 **대표적인 대규모 데이터셋**으로 네트워크 전체를 **사전 학습**  
- 모서리·텍스처·패턴 등 **저수준 feature**를 자동 추출하도록 가중치 학습  
- 다양한 downstream Task에서 **기초 표현(feature extractor)**으로 재사용 가능  

<br>

**Fine-tuning**

- Pre-trained 모델의 **상위 레이어**(분류기 헤드 등)를 대상 Task 데이터로 **재학습**  
- 전체 또는 일부 레이어를 **낮은 학습률**로 미세 조정해 빠르게 적응  
- 최소한의 학습 비용으로 Task-specific 성능을 **극대화**

<br>
<br>

### Step 5: Big Model & Zero/Few shot

> **추가 학습 없이** 단일 거대 모델만으로 모든 작업을 처리하는 단계. **(SW3.0)**   
> **0-shot** 또는 **few-shot** 예시만으로 **Task**를 지시하면 즉시 결과를 생성한다.  
> 주로 텍스트 모델에 적용되며, 대표 사례로 **GPT-3**가 있다.  
{: .prompt-info }

![Step-5](/assets/img/DL-Step5.png){: width="800" .center}

- 태스크별 **별도 모델 학습** 불필요  
- Task-specific 데이터 **수집·라벨링** 없이 **즉시 활용**  
- 거대한 단일 모델 하나로 **다양한 문제 해결**  
- **Pre-training & Fine-tuning** 대비 **추가 비용·시간** 대폭 절감

<br>

**Shot 개념**

- **Zero-shot**: 태스크 설명만 제공  
- **One-shot**: 예시 1개 제공  
- **Few-shot**: 소수의 예시(여러 개) 제공

<br>

**In-context learning vs Pre-training & Fine-tuning**

| 항목                | Pre-training & Fine-tuning | In-context Learning |
|--------------------|-----------------------------|---------------------|
| 추가 학습 여부      | 필요                        | 불필요              |
| 데이터 준비        | Task별 대규모 데이터 필요   | 태스크 설명·소량 예시만  |
| 모델 수            | Task별 각각 모델            | 하나의 통합 모델    |
| 적용 비용·시간     | 높음                        | 낮음                |

<br>
<br>
<br>

## Deep Learning Training

---

### 1. Perceptrons (퍼셉트론)

> 퍼셉트론은 **가중치**와 **편향**을 적용한 뒤 **활성화함수**로 **이진 출력**을 내는 **신경망의 최소 단위다.**   
> 이 기본 개념을 이해해야 **다층 퍼셉트론(MLP)과 역전파 학습 과정**을 명확히 파악할 수 있다.  
> (관련 용어는 [딥러닝 기본 용어 정리](/posts/deep-learning/)를 참고하자)  
{: .prompt-info }

![Perceptrons](/assets/img/DL-Perceptrons.png){: width="700" .center}


- 퍼셉트론의 연산은 **가중합(z)**을 구하고, 이를 **임계치**와 비교해 이진 출력을 생성하는 두 단계로 이루어진다.
- **threshold**(임계치) 보다 크면 1 아니면 0을 출력
- **bias**가 클수록 출력값이 쉽게 1이 된다.

<br>
<br>

### 2. Multi-Layer Perceptron, MLP (다층 퍼셉트론)

> 여러 **Perceptron**을 **layer**로 쌓아 복잡한 함수를 근사하는 신경망 구조  
> 은닉층을 거칠 떄마다 **비선형 활성화 함수**를 적용해 **복잡한 패턴을 학습할 수 있다.**  
> **입력층 -> 은닉층 -> 출력층** 으로 구성되며 **Fully-Connected Layers** 라고도 불린다.
{: .prompt-info }

![MLP](/assets/img/MLP-1.png){: width="600" .center}

**구성 요소**

- **Input Layer (입력층)**: 원시 데이터를 받아들임  
- **Hidden Layers (은닉층)**: 여러 퍼셉트론을 조합해 특징(feature) 추출  
- **Output Layer (출력층)**: 최종 예측값을 생성  
- **Weights (가중치)**: 각 뉴런 연결의 **중요도**를 결정하는 파라미터. 학습을 통해 최적값으로 조정됨  
- **Bias (편향)**: 뉴런이 활성화되기 위한 **기준점**을 이동시키는 상수항. 학습을 통해 값이 조정됨  
- **Activation Function (활성화 함수)**: 뉴런 출력을 **비선형**으로 변환해 복잡한 패턴 학습을 가능하게 함  

<br>
<br>

### 3. Gradient Descent (경사 하강법)

> **경사 하강법**은 **손실 함수**를 최소화하기 위해 **가중치**와 **편향**을 반복적으로 **조금씩** 업데이트하며  
> 최적의 파라미터를 찾아가는 **반복 최적화 기법**이다.
{: .prompt-info }

![Gradient-Descent](/assets/img/Gradient-Descent.png){: width="600" .center}

**핵심 원리**

- **경사 하강법(Gradient Descent)**은 말 그대로 "**경사(기울기)를 따라 내려가는**" 알고리즘이다.
- **손실 함수(Loss function)**가 가장 작은 지점을 찾기 위해, 현재 지점에서의 **gradient**를 계산하여 조금씩 이동
- **손실 함수의 기울기는 미분으로 구하며**, 손실을 **줄이는 방향으로 파라미터를 조정한다.**
- 간단히 말하면 **가장 가파르게 내려가는 방향을 반복적으로 찾아 움직이며, 결국 최적의 지점으로 수렴**하게 된다.

<br>

**경사 하강법의 종류**

- **배치** 경사 하강법(Batch Gradient Descent)
    - 전체 데이터를 **한 번에 사용해 기울기를 계산**하고 일괄 업데이트를 수행한다.
    - **안정적인 수렴을 보장하지만, 데이터가 많으면 속도가 느려진다.**
- **확률적** 경사 하강법(Stochastic Gradient Descent, SGD)
    - 각 데이터를 **하나씩 사용**할 때마다 파라미터를 업데이트한다.
    - **업데이트 빈도가 높아 불안정할 수 있지만, 학습 속도가 빠르고 빠르게 최적의 근처로 접근할 수 있다.**
    - 볼록ㅏ지 않ㅡ 목ㅓ식을 가진 경우 **SGD**가 **GD**보다 더 낫다고 검증됨
- **미니배치** 경사 하강법(Mini-batch Gradient Descent)
    - 데이터를 **여러 개의 작은 배치로 나누어 사용**하며, 각 배치를 사용해 업데이트한다.
    - **안정성과 속도를 모두 잡을 수 있어 가장 많이 활용된다.**

<br>

**학습률(Learning Rate)의 중요성**

- **학습률**은 한 번에 얼마나 **큰 보폭으로 이동할지 결정**하는 값이다.
- 학습률이 너무 크면 **발산하고**, 너무 작으면 **수렴이 느려진다.**
- **적절한 학습률을 설정**하는 것이 **경사 하강법의 성공을 결정**짓는다.

<br>

**경사 하강법의 한계와 개선 방법**

- **문제점**
    - 지역 최적점(Local minima)에 빠질 수 있다.
    - 안장점(Saddle points) 근처에서 학습이 느려지거나 멈출 수 있다.

- **개선 방법**
    - **Momentum(모멘텀)**: **과거의 기울기**를 이용해 **관성을 추가하여 안장점이나 지역 최적점을 빠르게 탈출**한다.
    - **Adagrad, RMSProp, Adam** 등의 알고리즘은 **학습률을 자동 조정해 최적화를 빠르고 안정적으로 수행한다.**

<br>
<br>

### 4. Backpropagation (역전파)
 
> **Backpropagation**은 **출력층에서 입력층 방향**으로 오차를 전파시키며 각 층의 **가중치를 업데이트**  
> 이 과정을 반복해 **손실을 점진적으로 최소화한다.**
{: .prompt-info }

**등장 배경**

> **MLP** 같은 깊은 신경망 덕분에 이제는 선형적으로 구분되지 않는 데이터까지도 꽤 잘 분류할 수 있게 됐다.  
> 그런데 **신경망이 점점 깊어지면서 문제가 생겼다.** 층이 많아지면 **가중치의 수도 기하급수적으로 늘어나는데**   
> 기존 방식인 경사하강법으로는 이 엄청난 수의 가중치를 일일이 업데이트하려다 보면 연산량이랑 메모리가 너무 많이 든다는 거다. 
> 그래서 나온 아이디어가 바로 **역전파 알고리즘(Backpropagation Algorithm)**이다.

![Backpropagation](/assets/img/Backpropagation.png){: width="700" .center}

**Feed Forward (순전파)**

> **입력층에서 출력층 방향으로 연산이 진행되는 순방향 신경망**(Feed Forward Neural Network)이다.  
> 연산이 **앞으로만 이동하는 과정을 순전파(Feed Forward)라고 부른다.**

![Feed-Forward](/assets/img/Feed-Forward.gif){: width="700" .center}

**Backpropagation (역전파)**

> 역전파의 핵심 아이디어는 사실 생각보다 간단하다. 결국 원하는 건 **신경망의 손실을 줄이는** 건데   
> **출력층에서 발생한 오차를 입력층 방향으로 되돌려서 각 가중치의 책임을 묻는 거다.**  
> 이때 신경망의 가중치마다 얼마나 책임이 있는지 알아내기 위해서, **각 층마다 손실 함수의 미분값을 계산**한다.  
> **즉, 역전파는 단순히 오차를 거꾸로 전달하는 게 아니라**, 각 가중치가 손실에 얼마나 기여했는지를  
> 정확하게 계산해서 **경사 하강법이 필요한 기울기 정보를 빠르고 효율적으로 전달해주는 역할**을 한다.  
> 덕분에 **수많은 가중치를 한 번에, 그리고 효과적으로 업데이트**할 수 있는 거다.  
> 전체적으로 **순전파(Feed Forward)와 역전파(BackPropagation)**의 흐름을 살펴보면 다음과 같다.

![순전파-역전파](/assets/img/순전파-역전파.gif){: width="800" .center}