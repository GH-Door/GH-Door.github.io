---
layout: post
title: "[빅분기] 필기 Chapter 03. 데이터 수집 및 저장"
description: "[빅데이터분석기사] 필기 'Chapter 03. 데이터 수집 및 저장' 내용을 정리했습니다."
author: "DoorNote"
date: 2025-03-17 10:00:00 +0900
permalink: /big-data-3/
categories:
    - 자격증
    - 빅데이터분석기사
tags: [빅데이터분석기사, PART 01. 빅데이터 분석 기획]
comments: true
pin: false # 고정핀
math: true
mermaid: true
image: /assets/img/빅데이터분석기사.png
---

## 들어가며

<blockquote style="background: #E1F5Fe; color: #2e2e2ec4; padding: 1rem; margin: 1.5rem 0; border-radius: 8px;">
**빅데이터분석기사 필기 시험**을 준비하며 공부한 내용을 **Chapter** 별 핵심 내용 기준으로 정리한 내용입니다.<br>
교재는 [이기적-빅데이터분석기사 필기-2024](https://search.shopping.naver.com/book/catalog/41869053620?cat_id=50010601&frm=PBOKPRO&query=%EC%9D%B4%EA%B8%B0%EC%A0%81+%EB%B9%85%EB%8D%B0%EC%9D%B4%ED%84%B0%EB%B6%84%EC%84%9D%EA%B8%B0%EC%82%AC+%ED%95%84%EA%B8%B0&NaPm=ct%3Dm88q4ot4%7Cci%3D23875283d14200743013ebc63ca27206ecd57363%7Ctr%3Dboknx%7Csn%3D95694%7Chk%3Dd90241b163c19c2f3cce4b072ef86e71d42f9d26)로 공부했습니다.   
</blockquote>

<br>
<br>

## 01. 데이터 수집 및 전환

---

### 1. 데이터 수집

<blockquote style="background: #E1F5Fe; color: #2e2e2ec4; padding: 1rem; margin: 1.5rem 0; border-radius: 8px;">
데이터 처리 시스템에 들어갈 데이터를 모으는 과정으로 여러 장소에 있는 데이터를 한 곳으로 모으는 것
</blockquote>

#### **1-1) 내 외부 데이터 수집**

- **내부 데이터**는 조직 내부의 **서비스 시스템, 네트워크 및 서버 장비, 마케팅 관련 시스템** 등으로부터 생성되는 데이터를 말한다.
- **외부 데이터**는 다양한 **소셜 데이터, 특정 기관 데이터, M2M 데이터, LOD** 등으로 나눌 수 있다.

<br>

| 위치 | 원천 시스템 | 종류 |
|:---:|:---:|:---|
| **내부 데이터** | **서비스 시스템** | - ERP, CRM, KMS, 포탈, 원장정보시스템<br>- 인증/과금 시스템, 거래시스템 등 |
|  | **네트워크 및 서버 장비** | - 백본, 방화벽, 스위치, IPS, IDS 서버 장비 로그 등 |
|  | **마케팅 데이터** | - VOC 접수 데이터, 고객 포털 시스템 등 |
| **외부 데이터** | **소셜 데이터** | - 제품 리뷰 커뮤니티, 게시판, 페이스북 등 |
|  | **특정 기관 데이터** | - 정책 데이터, 토론 사이트 |
|  | **M2M 데이터** | - 센서 데이터, 장비 발생 로그 |
|  | **LOD** | - 경제, 의료, 지역 정보, 공공 정책, 과학, 교육, 기술 등 |

<br>

#### **1-2) 데이터 수집 기술**

<br>

**정형 데이터**
- ETL: 데이터를 추출 및 가공하여 데이터 웨어하우스에 저장하는 기술
- FTP: TCP/IP나 UDP 프로토콜을 통해 원격지 시스템으로부터 파일을 송수신하는 기술
- API: 솔루션 제조사 및 3rd party 소프트웨어로 제공되는 도구
- DBToDB: 데이터베이스 관리자시스템 간 데이터를 동기화 또는 전송하는 방법
- 스쿱(Sqoop): 관계형 데이터베이스와 하둡 간 데이터를 전송하는 방법

<br>

**비정형 데이터**
- 크롤링: 웹 사이트로부터 소셜 네트워크 정보, 뉴스, 게시판 등으로부터 정보를 수집
- RSS: XML 기반으로 정보를 배포하는 프로토콜
- Open API: 응용 프로그램을 통해 실시간으로 데이터를 수신할 수 있도록 공개된 API
- 척와: 분산 시스템으로부터 데이터를 수집, 하둡 파일 시스템에 저장 실시간으로 분석할 수 있는 기능을 제공
- 카프카: 대용량 실시간 로그처리를 위한 분산 스트리밍 플랫폼 기술

<br>

**반정형 데이터**
- 플럼: 분산 환경에서 대량의 로그 데이터를 수집 전송하고 분석하는 기능을 제공
- 스크라이브: 다수의 수집 대상 서버로부터 실시간으로 데이터를 수집, 분산 시스템에 데이터를 저장하는 기능
- 센싱: 센서로부터 수집 및 생성된 데이터를 네트워크를 통해 활용하여 수집하는 기능을 제공
- 스트리밍: 네트워크를 통해 센서 데이터 및 오디오, 비디오 등 의 미디어 데이터를 실시간으로 수집하는 기술

<br>

**ETL**

**ETL**은 **추출(Extract), 변환(Transform), 적재(Load)의 3단계 프로세스**로 구성

| 프로세스 | 내용 |
|:---:|:---|
| 데이터 추출 | - 하나 또는 그 이상의 데이터 **원천으로부터** 데이터를 획득 |
| 데이터 변환 | - 목표로 하는 형식이나 구조로 데이터를 변환<br>- 데이터 정제, 변환, 표준화, 통합 등을 진행 |
| 데이터 적재 | - 변환이 완료된 데이터를 특정 목표 시스템에 저장 |

<br>

<img src="/assets/img/ETL-Process.jpg" width="80%">

<br>

**FTP**

1. **FTP**는 대량의 파일을 네트워크를 통해 주고받을 떄 사용되는 파일 전송 서비스

2. **FTP 특징**

    - 인터넷 프로토콜인 **TCP/IP 위에서 동작한다.**
    - 서버와 클라이언트를 먼저 연결하고 이후에 데이터 파일을 전송
    - **FTP** 서비스를 제공하는 서버와 접속하는 클라이언트 사이에 두 개의 연결을 생성
    - 사용자 계정 및 암호 등의 정보나 파일 전송 명령 및 결과 등은 데이터 제어 연결에서<br>
    - 이후 실제 파일 송수신 작업은 데이터 전송 연결에서 처리된다.

<br>

<img src="/assets/img/ftp.png" width="80%">

<br>

**정형 데이터** 수집을 위한 **아파치 스쿱**

- **관계형 데이터 스토어** 간에 **대량 데이터**를 효과적으로 전송하기 위해 구현된 도구
- **커넥터**를 사용하여 My*SQL*, Oracle, MS SQL 등 관계형 데이터베이스의 데이터를 하둡 파일시스템으로 수집
- **Sqoop**: ***SQL*** + ***Hadoop***
- **데이터 가져오기/내보내기** 과정을 **맵리듀스**를 통해 처리하기 때문에 벙렬처리가 가능하고 **장애에도** 강한 특징을 갖는다.

<br>

<img src="/assets/img/Sqoop.jpeg" width="80%">

<br>

**아파치 플럼 특징**

- 플럼은 로그 데이터 수집과 네트워크 트래픽 데이터, 소셜 미디어 데이터, 이메일 메시지 등 대량의 이벤트 데이터 전송을 위해 사용된다.

| 특징 | 설명 |
|:---:|:---|
| 신뢰성 | - 장애 시 로그 데이터의 유실 없이 전송을 보장 |
| 확장성 | - 수평 확장이 가능하며 분산 수집 가능한 구조 |
| 효율성 | - 커스터마이징 가능하면서 고성능을 제공 |

<br>

<img src="/assets/img/Flume.png" width="100%">

<br>
<br>

### 2. 데이터 유형 및 속성 파악

#### **2-1) 데이터 수집 세부 계획 작성**

<blockquote style="background: #E1F5Fe; color: #2e2e2ec4; padding: 1rem; margin: 1.5rem 0; border-radius: 8px;">
데이터 **유형, 위치, 크기, 보관방식, 수집주기, 확보비용, 데이터 이관 절차를 조사하여 세부 계획서**를 작성  
</blockquote>

<br>

#### **2-2) 데이터 수집 세부 계획 작성**

<br>

**데이터 유형**

- 크게 정형, 반정형, 비정형 으로 나뉜다.

| 유형 | 특징 | 종류 |
|:---:|:---|:---|
| 정형 데이터 | - **정형화된 스키마**를 가진 데이터 | RDB, File |
| 반정형 데이터 | - **메타 구조**를 가지는 데이터 | HTML, XML, JSON, RSS, 웹 로그 |
| 비정형 데이터 | - **이미지나 동영상**으로 존재하는 데이터 | 동영상, 이미지, 텍스트 |

<br>

**데이터 위치**

- 수집 데이터의 **원천에 따라 내부 데이터와 외부 데이터**로 구분할 수 있다.

| 위치 | 특징 | 분석 가치 |
|:---:|:---|:---|
| 내부 데이터 | - 대부분 **정형 데이터**로 존재<br>- 비용 및 데이터 수집 난이도가 낮다.<br>- 데이터 담당자와 협의가 원할 | 보통 |
| 외부 데이터 | - 대부분 **반정형, 비정형 데이터**로 존재<br>- 비용 및 데이터 수집 난이도가 높다.<br>- 외부 데이터 담당자와 의사소통이 어렵다. | 높음 |

<br>
<br>

### 3. 데이터 비식별화

<blockquote style="background: #E1F5Fe; color: #2e2e2ec4; padding: 1rem; margin: 1.5rem 0; border-radius: 8px;">
개인정보 **비식별화**는 개인정보를 식별할 수 있는 값들을 몇 가지 정해진 규칙으로 대체하거나 사람의 판단에 따라 가공하여 개인을 **알아볼 수 없도록 하는** 조치를 말한다.
</blockquote>

- 식별자: 개인을 고유하게 식별할 수 있는 정보
- 속성자: 개인에 대한 추가적인 정보

<br>

#### **3-1) 비식별 조치 방법**

<br>

| 처리기법 | 세부기술 |
|:---|:---|
| 가명처리 | 1. 휴리스틱 가명화<br>2. 암호화<br>3. 교환방법 |
| 총계처리 | 1. 총계처리<br>2. 부분총계<br>3. 라운딩<br>4. 재배열 |
| 데이터 삭제 | 1. 식별자 삭제<br>2. 식별자 부분삭제<br>3. 레코드 삭제<br>4. 식별요소 전부삭제 |
| 데이터 범주화 | 1. 감추기<br>2. 랜덤 라운딩<br>3. 범위 방법<br>4. 제어 라운딩 |
| 데이터 마스킹 | 1. 임의 잡음 추가<br>2. 공백과 대체 |

<br>

**장점**

| 처리기법 | 내용 |
|:---:|:---|
| **가명처리** | 데이터의 번형 또는 변질 수준이 적다. |
| **총계처리** | 민감한 수치 정보에 대하여 비식별 조치가 가능하며, 통계분석용 데이터셋 작성에 유리 |
| **데이터 삭제** | 개인 식별요소의 전부 및 일부 삭제 처리가 가능하다. |
| **데이터 범주화** | 통계형 데이터 형식이므로 다양한 분석 및 가공 가능 |
| **데이터 마스킹** | 개인 식별 요소를 제거하는 것이 가능하며, 원 데이터 구조에 대한 번형이 적다. |

<br>

**단점**

| 처리기법 | 내용 |
|:---:|:---|
| **가명처리** | 대체 값 부여 시에도 식별 가능한 고유 속성이 계속 유지된다. |
| **총계처리** | 정밀 분석이 어려우며, 집계 수량이 적을 경우 추론에 의한 식별이 가능 |
| **데이터 삭제** | 분석의 다양성과 분석 결과의 유효성, 신뢰성이 저하된다. |
| **데이터 범주화** | 정확한 분석결과 도출이 어려우며, 데이터 범위 구간이 좁혀질 경우 추론 가능성이 있다. |
| **데이터 마스킹** | 마스킹을 과도하게 적용할 경우 데이터 필요 목적에 활용하기 어려우며 마스킹 수준이 낮을 경우 특정한 값에 대한 추론이 가능하다. |

<br>

#### **3-2) 적정성 평가**

<blockquote style="background: #E1F5Fe; color: #2e2e2ec4; padding: 1rem; margin: 1.5rem 0; border-radius: 8px;">
**적정성 평가** 시 프라이버시 보호 모델 중 최소한의 수단으로 ***k-익멱성***을 활용하며, 필요시 추가적인 **평가모델(*l-다양성*, *t-근접성*)**을 활용한다.
</blockquote>

**프라이버시 보호 모델**

| 기법 | 의미 | 적용 |
|:---:|:---|:---|
| ***k-익명성*** | - 특정인임을 **추론**할 수 있는지 여부를 검토<br>- 일정 확률수준 이상 비식별 되도록 하는 기법 | - 동일한 값을 가진 레코드를 ***k***개 이상으로 하며<br>- 이 경우 특정 개인을 식별할 확률은 ***1/k***다. |
| ***l-다양성*** | - 특정인 추론이 안된다고 해도<br>- 민감한 정보의 다양성을 높여 추론 가능성을 낮추는 기법 | - 각 레코드를 ***l***개 이상의 다양성을 가지도록 하여<br>- 동질성 또는 배경지식 등에 의한 추론을 방지한다. |
| ***t-근접성*** | - ***l-다양성***뿐만 아니라, 민감한 정보의 분포를 낮추어<br>- 추론 가능성을 더욱 낮추는 기법 | - 전체 데이터 집합의 정보 분포와 특정 정보의<br>- 분포 차이를 ***t***이하로 하여 추론을 방지 |

<br>

1. ***k-익명성***
    - ***k-익명성***은 주어진 데이터 **집합에서 같은 값이 적어도 k개 이상 존재하도록 하여 쉽게 다른 정보로 결합할 수 없도록 한다.**

2. ***l-다양성***
    - ***k-익명성***에 대한 두 가지 공격, 즉 **동질성 공격 및 배경지식에 의한 공격을 방어하기 위한 모델**

3. ***t-근접성***
    - ***t-근접성***은 정보의 **분포를 조정하여 정보가 특정 값으로 쏠리거나 유사한 값들이 뭉치는 경우를 방지**하는 방법

<br>
<br>

### 4. 데이터 품질 검증

<blockquote style="background: #E1F5Fe; color: #2e2e2ec4; padding: 1rem; margin: 1.5rem 0; border-radius: 8px;">
비즈니스 목표에 부합한 데이터 분석을 위해 가치성, 정확성, 유용성 있는 데이터를 확보하고, 신뢰성 있는 데이터를 유지하는 데 필요한 관리 활동이다.
</blockquote>

#### **4-1) 데이터 품질**

**정형 데이터** 품질 기준

- **정형 데이터**에 대한 품질 기준은 **완전성, 유일성, 일관성, 유효성, 정확성** **5개**의 기준으로 나눌 수 있다.
    - **완전성**: 필수항목에 누락이 없어야 한다.
    - **유일성**: 데이터가 지켜야할 구조, 값, 표현되는 형태가 일관되게 정의되고, 서로 일치해야 한다.
    - **유효성**: 데이터 항목은 정해진 데이터 유효범위 및 도메인을 충족해야 한다.
    - **정확성**: 실세계에 존재하는 객체의 표현 값이 정확히 반영되어야 한다.

<br>

**비정형 데이터** 품질 기준

- **비정형** 컨텐츠 자체에 대한 품질 기준은 컨텐츠 유형에 따라 다소 다를 수 있다.
    - **기능성**: 명시적 요구와 내재된 요구를 만족하는 기능을 제공하는 정도
    - **신뢰성**: 규정된 신뢰 수준을 유지하거나, 오류를 방지할 수 있도록 하는 정도
    - **사용성**: 사용자에 의해 이해되고, 선호될 수 있게 하는 정도이다.
    - **효율성**: 자원의 양에 따라 요구된 성능을 제공하는 정도
    - **이식성**: 다양한 환경과 상황에서 실행될 가능성

<br>

#### **4-2) 데이터 품질 진단 기법**

**정형 데이터 품질 진단**

1. 메타 데이터 수집 및 분석
2. 칼럼 속성 분석
3. 누락 값 분석
4. 값의 허용 범위 분석
5. 허용 값 목록 분석
6. 문자열 패턴 분석
7. 날짜 유형 분석
8. 기타 특수 도메인 분석
9. 유일 값 분석
10. 구조 분석

<br>

**비정형 데이터 품질 진단**

- **비정형 데이터**의 품질 진단은 품질 세부 기준을 정하여 항목별 **체크리스트**를 작성하여 진단한다.

<br>
<br>
<br>

## 02. 데이터 적재 및 저장

---

### 1. 데이터 적재

<blockquote style="background: #E1F5Fe; color: #2e2e2ec4; padding: 1rem; margin: 1.5rem 0; border-radius: 8px;">
적재할 데이터의 유형과 실시간 처리 여부에 따라 **관계형 데이터베이스, HDFS**를 비롯한 **분산파일시스템, NoSQL** 저장 시스템에 데이터를 적재할 수 있다.
</blockquote>

#### **1-1) 데이터 적재 도구**

<br>

| 데이터 수집 도구 | 설명 |
|:---:|:---|
| 플루언티드<br>(Fluentd) | - 트레저 데이터에서 개발된 크로스 플랫폼 오픈 소스 데이터 수집 소프트웨어 |
| 플럼<br>(Flume) | - 많은 양의 로그 데이터를 효율적으로 수집, 취합, 이동하기 위한 분산형 소프트웨어 |
| 스크라이브<br>(Scribe) | - 수많은 서버로부터 실시간으로 스트리밍되는 로그 데이터를 집약시키기 위한 서버 |
| 로그스태시<br>(Logstash) | - 다양한 소스에서 데이터를 수집하여 변환한 후 자주 사용되는 저장소 |

<br>

#### **1-2) 데이터 적재 완료 테스트**

**정형 데이터**
- 정형 데이터인 경우에는 **테이블의 개수와 속성의 개수 및 데이터 타입의 일치여부, 레코드 수 일치 여부**가 체크리스트가 될 수 있다.

**반정형, 비정형 데이터**
- 반정형이나 비정형 데이터인 경우에는 **원천 데이터의 테이블이 목적지 저장시스템에 맞게 생성되었는지, 레코드 수가 일치하는지** 등이 체크리스트에 포함될 수 있다.

<br>
<br>

### 2. 데이터 저장

#### **2-1) 분산 파일 시스템**

1. **하둡** 분산 파일 시스템**(HDFS)**
    - **HDFS**는 대용량 파일을 클러스터에 여러 블록으로 분산하여 저장하며, 블록들은 마지막 블록을 제외하고 모두 크기가 동일하다.
    - **HDFS**는 **마스터(Master)** 하나와 여러 개의 **슬레이브(Slave)**로 클러스터링 되어 구성된다.

2. **구글** 파일 시스템**(GFS)**
    - **GFS**는 엄청나게 많은 데이터를 보유해야 하는 구글의 핵심 데이터 스토리지와 구글 검색 엔진을 위해 최적화된 분산 파일 시스템
    - **마스터(Master)**, 청크 서버(Chunk Server), 클라이언트로 구성된다.

3. **NoSQL**
    - **NoSQL** 데이터베이스는 전통적인 관계형 데이터베이스보다 유연한 데이터의 저장 및 검색을 위한 매커니즘을 제공
    - 대규모 데이터를 처리하기 위한 **확장성, 가용성 및 높은 성능을 제공하며** 빅데이터 처리와 저장을 위한 플랫폼으로 활용
    - **SQL** 계열 쿼리를 지원하는 데이터베이스도 있어 **"Not Only SQL"**로 불리기도 한다.

<br>

**NoSQL**의 데이터 모델

> NoSQL의 데이터 저장 방식에 따라 **키-값 구조, 칼럼기반 구조, 문서기반** 구조로 구분할 수 있다.

<br>

**키-값(Key-Value)** 데이터베이스

- 데이터를 키와 그에 해당하는 값의 쌍으로 저장하는 데이터 모델에 기반을 둔다.
- **관계형 데이터베이스보다 확장성이** 뛰어나고 질의 응답시간이 빠르다.

**열기반(칼럼기반, Column-oriented)** 데이터베이스

- 데이터를 로우가 아닌 **칼럼기반**으로 저장하고 처리하는 데이터베이스를 말한다.
- **여러개의 노드로 분할되어** 저장되어 관리한다.

**문서기반(Document-oriented)** 데이터베이스

- 문서 형식의 정보를 저장, 검색, 관리하기 위한 데이터베이스다.
- **키-값 데이터베이스보다** 문서의 내부 구조에 기반을 둔 **복잡한 형태의 데이터 저장을 지원하고 이에 따른 최적화가 가능하다는 장점이 있다.**
- 대표적으로 **MongoDB, SimpleDB, CouchDB**가 있다.

